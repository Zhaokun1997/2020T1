{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code for hiding seaborn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "from sklearn import metrics, pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(training_path, testing_path):\n",
    "    df_train = pd.read_csv('training.csv')\n",
    "    df_test = pd.read_csv('test.csv')\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_data(df_train, df_test, stop_words):\n",
    "    # remove irrelevant items\n",
    "    df_train = df_train.loc[df_train['topic'] != 'IRRELEVANT']\n",
    "    df_test = df_test.loc[df_test['topic'] != 'IRRELEVANT']\n",
    "\n",
    "    # add topic codes for each topic\n",
    "    # generate topic codes for each topic\n",
    "    topic_codes = {\n",
    "        'ARTS CULTURE ENTERTAINMENT': 0,\n",
    "        'BIOGRAPHIES PERSONALITIES PEOPLE': 1,\n",
    "        'DEFENCE': 2,\n",
    "        'DOMESTIC MARKETS': 3,\n",
    "        'FOREX MARKETS': 4,\n",
    "        'HEALTH': 5,\n",
    "        'MONEY MARKETS': 6,\n",
    "        'SCIENCE AND TECHNOLOGY': 7,\n",
    "        'SHARE LISTINGS': 8,\n",
    "        'SPORTS': 9\n",
    "    }\n",
    "\n",
    "    # Category mapping\n",
    "    df_train['topic_codes'] = df_train['topic']\n",
    "    df_train = df_train.replace({'topic_codes': topic_codes})\n",
    "    df_test['topic_codes'] = df_test['topic']\n",
    "    df_test = df_test.replace({'topic_codes': topic_codes})\n",
    "\n",
    "    # remove stop words\n",
    "    df_train['content_parsed'] = df_train['article_words']\n",
    "    df_test['content_parsed'] = df_test['article_words']\n",
    "\n",
    "    for stop_word in stop_words:\n",
    "        regex_stopword = r'\\b' + ',' + stop_word + r'\\b'\n",
    "        regex2_stopword = r'\\b' + stop_word + ',' + r'\\b'\n",
    "        df_train['content_parsed'] = df_train['content_parsed'].str.replace(regex_stopword, '')\n",
    "        df_train['content_parsed'] = df_train['content_parsed'].str.replace(regex2_stopword, '')\n",
    "        df_test['content_parsed'] = df_test['content_parsed'].str.replace(regex_stopword, '')\n",
    "        df_test['content_parsed'] = df_test['content_parsed'].str.replace(regex2_stopword, '')\n",
    "    \n",
    "    # add article length and id information\n",
    "    df_train['article_length'] = df_train['content_parsed'].str.len()\n",
    "    df_test['article_length'] = df_test['content_parsed'].str.len()\n",
    "    df_train['id'] = 1\n",
    "    df_test['id'] = 1\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extract(df_train, df_test, nb_features):\n",
    "    # for each class, get the most frequent  words as feature vectors\n",
    "    words_set_train = set()\n",
    "    for i in range(1, 11, 1):\n",
    "        bag = df_train[df_train['topic_codes'] == i]['content_parsed']\n",
    "        total_text = \"\"\n",
    "        for text in bag:\n",
    "            total_text += (text + \",\")\n",
    "        temp_set = set(pd.value_counts(total_text.split(\",\"))[0:nb_features].keys())\n",
    "        words_set_train = words_set_train.union(temp_set)\n",
    "\n",
    "    words_set_test = set()\n",
    "    for i in range(1, 11, 1):\n",
    "        bag = df_test[df_test['topic_codes'] == i]['content_parsed']\n",
    "        total_text = \"\"\n",
    "        for text in bag:\n",
    "            total_text += (text + \",\")\n",
    "        temp_set = set(pd.value_counts(total_text.split(\",\"))[0:100].keys())\n",
    "        words_set_test = words_set_test.union(temp_set)\n",
    "\n",
    "    # get the intersection of two feature words(including training feature words and testing feature words)\n",
    "    # and use the intersection as our feature vector X\n",
    "    words_set = words_set_train.intersection(words_set_test)\n",
    "    words_list = sorted(list(words_set))\n",
    "#     print(\"words feature numbers : \", len(words_list), \"gives the best performance!\")\n",
    "#     print(\"our words feature is : \", words_set)\n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize_data(df_train, df_test, words_list):\n",
    "    # regularize training data\n",
    "    new_content_train = []\n",
    "    for row in df_train['content_parsed']:\n",
    "        temp_row = row.split(\",\")\n",
    "        new_row = []\n",
    "        for word in temp_row:\n",
    "            if word in words_list:\n",
    "                new_row.append(word)\n",
    "        new_str = \",\".join(new_row)\n",
    "        new_content_train.append(new_str)\n",
    "    df_train['content_parsed_2'] = new_content_train\n",
    "\n",
    "    # regularize testing data\n",
    "    new_content_test = []\n",
    "    for row in df_test['content_parsed']:\n",
    "        temp_row = row.split(\",\")\n",
    "        new_row = []\n",
    "        for word in temp_row:\n",
    "            if word in words_list:\n",
    "                new_row.append(word)\n",
    "        new_str = \",\".join(new_row)\n",
    "        new_content_test.append(new_str)\n",
    "    df_test['content_parsed_2'] = new_content_test\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "#     \"\"\"\n",
    "#     对数损失度量（Logarithmic Loss  Metric）的多分类版本。\n",
    "#     :param actual: 包含actual target classes的数组\n",
    "#     :param predicted: 分类预测结果矩阵, 每个类别都有一个概率\n",
    "#     \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_process_for_MultinomialNB_model(X_train, y_train, X_test, y_test, alpha=1.0):\n",
    "    # # get prior probability (probability of each topic is not uniformed)\n",
    "    # prior_pro = dict(df_train.groupby('topic_codes').count()['topic'] / df_train.shape[0])\n",
    "    # class_prior = list(prior_pro.values())\n",
    "#     # do cross validation:\n",
    "#     # create a k-fold croos validation iterator of k folds\n",
    "#     cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "#     # by default the score used is the one returned by score method of the estimator (accuracy)\n",
    "#     scores = cross_val_score(clf, X_train, y_train, cv=cv)\n",
    "#     cv_mean_score = np.mean(scores)\n",
    "#     print(\"Mean score: {0:.3f}\").format(cv_mean_score)\n",
    "#     return cv_mean_score\n",
    "#     return clf.score(X_test, y_test)\n",
    "\n",
    "    \n",
    "    # create model and predict result\n",
    "    print(\"MNB model created...:\")\n",
    "    clf = MultinomialNB(alpha=alpha, fit_prior=True, class_prior=None)    \n",
    "    model = clf.fit(X_train, y_train)\n",
    "    predicted_y = clf.predict(X_test)\n",
    "    predictions = model.predict_proba(X_test)\n",
    "    print(\"logloss: %0.3f \" % multiclass_logloss(y_test, predictions))\n",
    "    print(\"accuracy : \",clf.score(X_test, y_test))\n",
    "#     print(accuracy_score(y_test, predicted_y[:,1]))\n",
    "#     print(precision_score(y_test, predicted_y, average='macro'))\n",
    "#     print(recall_score(y_test, predicted_y, average='macro'))\n",
    "#     print(f1_score(y_test, predicted_y, average='macro'))\n",
    "    print(\"here below is classification report:\")\n",
    "    print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_process_for_MultinomialLR_model(X_train, y_train, X_test, y_test, C=1.0):\n",
    "    # create model and predict result\n",
    "    print(\"MLR model created...:\")\n",
    "    clf = LogisticRegression(C=C, solver='lbfgs', multi_class='multinomial')\n",
    "    model = clf.fit(X_train, y_train)\n",
    "    predicted_y = clf.predict(X_test)\n",
    "    predictions = clf.predict_proba(X_test)\n",
    "    \n",
    "    \n",
    "    print(\"predictions : \",predictions)\n",
    "    print(\"logloss: %0.3f \" % multiclass_logloss(y_test, predictions))\n",
    "    print(\"accuracy : \",clf.score(X_test, y_test))\n",
    "#     print(accuracy_score(y_test, predicted_y[:,1]))\n",
    "#     print(precision_score(y_test, predicted_y, average='macro'))\n",
    "#     print(recall_score(y_test, predicted_y, average='macro'))\n",
    "#     print(f1_score(y_test, predicted_y, average='macro'))\n",
    "    print(\"here below is classification report:\")\n",
    "    print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/steve/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# get stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(stopwords.words('english'))  # get stop words\n",
    "# print(stop_words)\n",
    "\n",
    "# do some data cleaning and pre-processing\n",
    "df_train, df_test = load_data(\"training.csv\", \"test.csv\")\n",
    "df_train, df_test = pre_processing_data(df_train, df_test, stop_words)\n",
    "\n",
    "# nb_features = []\n",
    "# accuracy_list = []\n",
    "# for nb in range(50, 200, 5):\n",
    "#     feature_words = features_extract(df_train, df_test, nb)\n",
    "#     df_train, df_test = regularize_data(df_train, df_test, feature_words)\n",
    "#     accuracy = training_process_for_MultinomialNB_model(df_train, df_test)\n",
    "#     nb_features.append(nb)\n",
    "#     accuracy_list.append(accuracy)\n",
    "\n",
    "best_nb = 65\n",
    "feature_words = features_extract(df_train, df_test, best_nb)\n",
    "df_train, df_test = regularize_data(df_train, df_test, feature_words)\n",
    "\n",
    "\n",
    "# create bag of words\n",
    "text_data_train = np.array(df_train['content_parsed_2'])\n",
    "count_train = CountVectorizer()\n",
    "bag_of_words_train = count_train.fit_transform(text_data_train)\n",
    "\n",
    "text_data_test = np.array(df_test['content_parsed_2'])\n",
    "count_test = CountVectorizer()\n",
    "bag_of_words_test = count_test.fit_transform(text_data_test)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(bag_of_words_train.toarray(), \n",
    "#                                                 np.array(df_train['topic_codes']), \n",
    "#                                                 test_size=0.15, \n",
    "#                                                 random_state=0)\n",
    "    \n",
    "# Create feature matrix and target, train model\n",
    "X_train = bag_of_words_train.toarray()\n",
    "y_train = np.array(df_train['topic_codes'])\n",
    "X_test = bag_of_words_test.toarray()\n",
    "y_test = np.array(df_test['topic_codes'])\n",
    "\n",
    "\n",
    "# plt.plot(nb_features, accuracy_list, label=\"accuracy_trend\", color='red')\n",
    "# plt.xlabel(\"nb_features\")\n",
    "# plt.ylabel(\"accuracy score\")\n",
    "# plt.xticks(nb_features)  # show x-coordinate with details\n",
    "# plt.legend()  # show label graphic\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR model created...:\n",
      "logloss: 0.655 \n",
      "accuracy :  0.7435897435897436\n",
      "here below is classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.33      0.29         3\n",
      "           1       0.82      0.60      0.69        15\n",
      "           2       1.00      0.92      0.96        13\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       0.54      0.40      0.46        48\n",
      "           5       0.69      0.79      0.73        14\n",
      "           6       0.65      0.77      0.70        69\n",
      "           7       0.25      0.33      0.29         3\n",
      "           8       0.88      1.00      0.93         7\n",
      "           9       0.98      0.98      0.98        60\n",
      "\n",
      "    accuracy                           0.74       234\n",
      "   macro avg       0.71      0.71      0.70       234\n",
      "weighted avg       0.74      0.74      0.74       234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training_process_for_MultinomialNB_model(X_train, y_train, X_test, y_test, alpha=1.0)\n",
    "# training_process_for_MultinomialLR_model(X_train, y_train, X_test, y_test, C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 6 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  36 | elapsed:    2.4s remaining:    2.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "\tnb__alpha: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  21 out of  36 | elapsed:    2.4s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  36 | elapsed:    2.4s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  36 | elapsed:    2.4s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  36 | elapsed:    2.4s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    2.5s finished\n"
     ]
    }
   ],
   "source": [
    "# create score fuction\n",
    "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# create pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# search parameters\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Grid Search Model Initialization\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=6)\n",
    "\n",
    "# fit Grid Search Model\n",
    "model.fit(X_train, y_train)\n",
    "# print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB model created...:\n",
      "logloss: 1.318 \n",
      "accuracy :  0.7478632478632479\n",
      "here below is classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.78      0.47      0.58        15\n",
      "           2       0.71      0.92      0.80        13\n",
      "           3       0.67      1.00      0.80         2\n",
      "           4       0.57      0.50      0.53        48\n",
      "           5       0.72      0.93      0.81        14\n",
      "           6       0.69      0.74      0.71        69\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.67      0.86      0.75         7\n",
      "           9       0.97      1.00      0.98        60\n",
      "\n",
      "    accuracy                           0.75       234\n",
      "   macro avg       0.58      0.64      0.60       234\n",
      "weighted avg       0.73      0.75      0.73       234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training_process_for_MultinomialNB_model(X_train, y_train, X_test, y_test, alpha=1.0)\n",
    "training_process_for_MultinomialNB_model(X_train, y_train, X_test, y_test, alpha=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 5 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  30 | elapsed:    1.6s remaining:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  30 | elapsed:    3.1s remaining:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  30 | elapsed:    3.1s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:    3.2s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  30 | elapsed:    3.8s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    3.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "\tlr__C: 0.1\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(solver='lbfgs',multi_class='multinomial')\n",
    "\n",
    "# create pipeline \n",
    "clf = pipeline.Pipeline([('lr', lr_model)])\n",
    "\n",
    "# search parameters\n",
    "param_grid = {'lr__C': [0.01, 0.1, 1.0, 10, 100]}\n",
    "\n",
    "# Grid Search Model Initialization\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=6)\n",
    "\n",
    "# fit Grid Search Model\n",
    "model.fit(X_train, y_train)\n",
    "# print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR model created...:\n",
      "predictions :  [[3.24386089e-08 8.68931887e-08 1.42268060e-09 ... 1.08354356e-08\n",
      "  1.52937581e-06 3.03303096e-09]\n",
      " [1.09829683e-09 2.63056358e-08 1.59822330e-10 ... 4.30331175e-11\n",
      "  1.62565673e-08 3.20432110e-12]\n",
      " [6.32184479e-05 2.00389476e-04 4.92442081e-06 ... 7.25583224e-05\n",
      "  4.56508053e-06 9.99628831e-01]\n",
      " ...\n",
      " [1.20296382e-10 3.60756916e-11 7.56610052e-16 ... 3.24590355e-11\n",
      "  9.44041924e-16 1.00000000e+00]\n",
      " [2.57985249e-06 2.75701348e-06 1.93898624e-07 ... 4.10725938e-06\n",
      "  1.20515613e-06 7.92717951e-07]\n",
      " [1.06881607e-02 8.27904479e-03 7.00471697e-03 ... 9.03275562e-03\n",
      "  5.48411277e-01 2.67142215e-02]]\n",
      "logloss: 0.550 \n",
      "accuracy :  0.7435897435897436\n",
      "here below is classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.33      0.25         3\n",
      "           1       0.90      0.60      0.72        15\n",
      "           2       1.00      0.92      0.96        13\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       0.53      0.35      0.42        48\n",
      "           5       0.71      0.86      0.77        14\n",
      "           6       0.64      0.78      0.70        69\n",
      "           7       0.33      0.33      0.33         3\n",
      "           8       0.88      1.00      0.93         7\n",
      "           9       0.98      0.98      0.98        60\n",
      "\n",
      "    accuracy                           0.74       234\n",
      "   macro avg       0.72      0.72      0.71       234\n",
      "weighted avg       0.75      0.74      0.74       234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training_process_for_MultinomialLR_model(X_train, y_train, X_test, y_test, C=1.0)\n",
    "training_process_for_MultinomialLR_model(X_train, y_train, X_test, y_test, C=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56651809, 3.        , 0.        ],\n",
       "       [0.84401838, 5.        , 1.        ],\n",
       "       [0.86852502, 7.        , 0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_y = np.array([0,1,0])\n",
    "labels = np.array([3, 5, 7])\n",
    "pre_p = np.array([[0.56651809,0.43348191],[0.15598162,0.84401838], [0.86852502,0.13147498]])\n",
    "\n",
    "result = np.c_[np.max(pre_p, axis=1), labels.T, pre_y.T]\n",
    "result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5)) #整个现实图（框架）的大小\n",
    "plt.plot(nb_features, accuracy_list, label=\"accuracy_trend\", color='red')\n",
    "plt.xlabel(\"nb_features\")\n",
    "plt.ylabel(\"accuracy score\")\n",
    "plt.xticks(nb_features)  # show x-coordinate with details\n",
    "plt.legend()  # show label graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = alt.Chart(df_train).mark_bar(size=50).encode(\n",
    "    x=alt.X(\"topic\"),\n",
    "    y=alt.Y(\"count():Q\", axis=alt.Axis(title='Number of articles')),\n",
    "    tooltip=[alt.Tooltip('count()', title='Number of articles'), 'topic'],\n",
    "    color='topic'\n",
    "\n",
    ")\n",
    "\n",
    "text = bars.mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    ").encode(\n",
    "    text='count()'\n",
    ")\n",
    "\n",
    "(bars + text).interactive().properties(\n",
    "    height=500, \n",
    "    width=700,\n",
    "    title = \"Number of articles in each topic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic = pd.DataFrame(df_train.groupby('topic').count()['id']).reset_index()\n",
    "\n",
    "bars = alt.Chart(df_topic).mark_bar(size=50).encode(\n",
    "    x=alt.X('topic'),\n",
    "    y=alt.Y('PercentOfTotal:Q', axis=alt.Axis(format='.0%', title='% of Articles')),\n",
    "    color='topic'\n",
    ").transform_window(\n",
    "    TotalArticles='sum(id)',\n",
    "    frame=[None, None]\n",
    ").transform_calculate(\n",
    "    PercentOfTotal=\"datum.id / datum.TotalArticles\"\n",
    ")\n",
    "\n",
    "text = bars.mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    "    #dx=5  # Nudges text to right so it doesn't appear on top of the bar\n",
    ").encode(\n",
    "    text=alt.Text('PercentOfTotal:Q', format='.1%')\n",
    ")\n",
    "\n",
    "(bars + text).interactive().properties(\n",
    "    height=500, \n",
    "    width=700,\n",
    "    title = \"% of articles in each topic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sns.distplot(df_train['article_length']).set_title('article_length distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('topic').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prior = list(prior_pro.values())\n",
    "\n",
    "X_train = bag_of_words_train.toarray()\n",
    "y_train = np.array(df_train['topic_codes'])\n",
    "\n",
    "X_test = bag_of_words_test.toarray()\n",
    "y_test = np.array(df_test['topic_codes'])\n",
    "\n",
    "\n",
    "clf = MultinomialNB(fit_prior=True, class_prior=None)\n",
    "model = clf.fit(X_train, y_train)\n",
    "predicted_y = model.predict(X_test)\n",
    "print(accuracy_score(y_test, predicted_y))\n",
    "print(precision_score(y_test, predicted_y, average='macro'))\n",
    "print(recall_score(y_test, predicted_y, average='macro'))\n",
    "print(f1_score(y_test, predicted_y, average='macro'))\n",
    "print(classification_report(y_test, predicted_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
